{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8Aj_SaJrsdEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e410ee-2dc5-429e-80db-7fafabf83933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Mounting the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_root='/content/drive/My Drive/Colab Notebooks/Chatbot'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Importing Relevant Libraries\n",
        "import json\n",
        "import string\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPWeZQmD5P-Q",
        "outputId": "70bd80cb-475c-4309-d6bd-d20b75eb2a1a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset (intents.json)\n",
        "data_file = open(data_root + '/general intents.json').read()\n",
        "data = json.loads(data_file)\n"
      ],
      "metadata": {
        "id": "8-pEoaZj562W"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating data_X and data_Y\n",
        "\n",
        "words = [] #For Bow model/vocabulary for patterns\n",
        "classes = [] #For Bow model/vocabulary for tags\n",
        "data_x = [] #For storing each pattern\n",
        "data_y = [] #For storing tag corresponding to each pattern in data_x\n",
        "\n",
        "#Iterating over all the intents\n",
        "for intent in data[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    tokens = nltk.word_tokenize(pattern) #tokenize each pattern\n",
        "    words.extend(tokens) #and append token to words\n",
        "    data_x.append(pattern) #appending pattern to data_x\n",
        "    data_y.append(intent[\"tag\"]) #appending associated tag to data_y\n",
        "\n",
        "    #Adding tag to the classes if it's not there already\n",
        "    if intent[\"tag\"] not in classes:\n",
        "      classes.append(intent[\"tag\"])\n",
        "\n",
        "#Initialising lemmatizer to get stem of words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#Lemmatize all the words in the vocab and convert them to lowercase\n",
        "#if words don't appear in punctuation\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "\n",
        "#Sorting the vocabulary and classes in alphabetical order and\n",
        "#taking the set to ensure not duplicates occur\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))"
      ],
      "metadata": {
        "id": "uSxGkS5f7ao-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting texts to numbers for Bag of Words model\n",
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "\n",
        "#Creating bag of words model\n",
        "for idx, doc in enumerate(data_x):\n",
        "  bow = []\n",
        "  text = lemmatizer.lemmatize(doc.lower())\n",
        "  for word in words:\n",
        "    bow.append(1) if word in text else bow.append(0)\n",
        "  \n",
        "  #Mark the index of class that the current pattern is associated to\n",
        "  output_row = list(out_empty)\n",
        "  output_row[classes.index(data_y[idx])] = 1\n",
        "\n",
        "  #Add the one hot encoded BoW and associated classes to training\n",
        "  training.append([bow,output_row])\n",
        "\n",
        "#Shuffle the data and convert it into an array\n",
        "random.shuffle(training)\n",
        "training = np.array(training,dtype=object)\n",
        "\n",
        "#Split the features and target labels\n",
        "train_x = np.array(list(training[:,0]))\n",
        "train_y = np.array(list(training[:,1]))\n"
      ],
      "metadata": {
        "id": "1OVP8RKZ15GT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Deep Neural Networds model\n",
        "model = Sequential()\n",
        "model.add(Dense(128,input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64,activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]),activation = 'softmax'))\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate = 0.01, decay = 1e-6)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer=adam, metrics = [\"accuracy\"])\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(x=train_x,y=train_y,epochs=150,verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GB8cIxR4HWy",
        "outputId": "5d16ba24-3ee9-4f8f-ad2f-8972826387e7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 128)               3712      \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,163\n",
            "Trainable params: 12,163\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/150\n",
            "1/1 [==============================] - 1s 589ms/step - loss: 1.0697 - accuracy: 0.5000\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0820 - accuracy: 0.3750\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9835 - accuracy: 0.6875\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.9633 - accuracy: 0.6250\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8059 - accuracy: 0.5625\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.8175 - accuracy: 0.6875\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7212 - accuracy: 0.7500\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5888 - accuracy: 0.8125\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5240 - accuracy: 0.8750\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5265 - accuracy: 0.8125\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5116 - accuracy: 1.0000\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3087 - accuracy: 0.9375\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2756 - accuracy: 0.9375\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2690 - accuracy: 1.0000\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2109 - accuracy: 0.9375\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1738 - accuracy: 1.0000\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1803 - accuracy: 1.0000\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1568 - accuracy: 1.0000\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0931 - accuracy: 1.0000\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0820 - accuracy: 1.0000\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0967 - accuracy: 1.0000\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0288 - accuracy: 1.0000\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0162 - accuracy: 1.0000\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0419 - accuracy: 1.0000\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0194 - accuracy: 1.0000\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0130 - accuracy: 1.0000\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0447 - accuracy: 1.0000\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.9397e-04 - accuracy: 1.0000\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.0974e-04 - accuracy: 1.0000\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0672 - accuracy: 0.9375\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0207 - accuracy: 1.0000\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.0587e-04 - accuracy: 1.0000\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.2501e-05 - accuracy: 1.0000\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1794e-04 - accuracy: 1.0000\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8288e-04 - accuracy: 1.0000\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1404e-04 - accuracy: 1.0000\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.8613e-04 - accuracy: 1.0000\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8880e-04 - accuracy: 1.0000\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.3140e-04 - accuracy: 1.0000\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1474e-04 - accuracy: 1.0000\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.6094e-04 - accuracy: 1.0000\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.3680e-04 - accuracy: 1.0000\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.5903e-04 - accuracy: 1.0000\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.9846e-05 - accuracy: 1.0000\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.4737e-05 - accuracy: 1.0000\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3985e-04 - accuracy: 1.0000\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.7804e-04 - accuracy: 1.0000\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8061e-04 - accuracy: 1.0000\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.1945e-05 - accuracy: 1.0000\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5655e-04 - accuracy: 1.0000\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0170 - accuracy: 1.0000\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0199e-04 - accuracy: 1.0000\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0971e-04 - accuracy: 1.0000\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1531e-05 - accuracy: 1.0000\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.4054e-04 - accuracy: 1.0000\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.9684e-05 - accuracy: 1.0000\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0115e-05 - accuracy: 1.0000\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.8937e-05 - accuracy: 1.0000\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.7401e-04 - accuracy: 1.0000\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9692e-04 - accuracy: 1.0000\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.6212e-05 - accuracy: 1.0000\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.2379e-04 - accuracy: 1.0000\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3730e-05 - accuracy: 1.0000\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.4090e-06 - accuracy: 1.0000\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.7686e-04 - accuracy: 1.0000\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8916e-05 - accuracy: 1.0000\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.8767e-05 - accuracy: 1.0000\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9223e-04 - accuracy: 1.0000\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0512e-05 - accuracy: 1.0000\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.5095e-05 - accuracy: 1.0000\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3504e-04 - accuracy: 1.0000\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3038e-04 - accuracy: 1.0000\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.2581e-05 - accuracy: 1.0000\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.9461e-05 - accuracy: 1.0000\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9338e-04 - accuracy: 1.0000\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6174e-04 - accuracy: 1.0000\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7018e-04 - accuracy: 1.0000\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.7178e-05 - accuracy: 1.0000\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5064e-05 - accuracy: 1.0000\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.0141e-04 - accuracy: 1.0000\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7074e-04 - accuracy: 1.0000\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.5606e-04 - accuracy: 1.0000\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.0967e-06 - accuracy: 1.0000\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.0956e-05 - accuracy: 1.0000\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0531e-04 - accuracy: 1.0000\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.1860e-04 - accuracy: 1.0000\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7283e-04 - accuracy: 1.0000\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.1483e-06 - accuracy: 1.0000\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4122e-05 - accuracy: 1.0000\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6741e-05 - accuracy: 1.0000\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6875e-04 - accuracy: 1.0000\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.2477e-04 - accuracy: 1.0000\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2060e-05 - accuracy: 1.0000\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8551e-04 - accuracy: 1.0000\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1740e-05 - accuracy: 1.0000\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7654e-05 - accuracy: 1.0000\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8327e-04 - accuracy: 1.0000\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4116e-04 - accuracy: 1.0000\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3381e-05 - accuracy: 1.0000\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.3128e-05 - accuracy: 1.0000\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3411e-07 - accuracy: 1.0000\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.2650e-05 - accuracy: 1.0000\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.1417e-05 - accuracy: 1.0000\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.2037e-06 - accuracy: 1.0000\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.6396e-04 - accuracy: 1.0000\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.7017e-05 - accuracy: 1.0000\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.8286e-06 - accuracy: 1.0000\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.6793e-05 - accuracy: 1.0000\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3394e-04 - accuracy: 1.0000\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0017 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6e6cccc7c0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing Input\n",
        "def clean_text(text):\n",
        "  tokens = nltk.word_tokenize(text) #Receives each text as an input and tokenizes it\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens] #Receives the token and then converts it into root form via lemmatizer\n",
        "  return tokens #Output is a list of words in their root form\n",
        "\n",
        "def bag_of_words(text,vocab):\n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab) #forming the Bag-of-Words Model\n",
        "  for w in tokens:\n",
        "    for idx,word in enumerate(vocab):\n",
        "      if word == w:\n",
        "        bow[idx] = 1 #Converts text into array using the model and input vocabulary\n",
        "  return np.array(bow) \n",
        "\n",
        "def pred_class(text,vocab,labels): \n",
        "#Takes text, vocabulary and labels as input and returns a list containing \n",
        "#a tag that corresponds to the highest probability\n",
        "  bow = bag_of_words(text,vocab)\n",
        "  result = model.predict(np.array([bow]))[0] # Extracting possibilities\n",
        "  thresh = 0.5\n",
        "  y_pred = [[indx,res] for indx,res in enumerate(result) if res > thresh]\n",
        "  y_pred.sort(key=lambda x: x[1],reverse = True) #Sorting by values of probability in decreasing order\n",
        "  return_list = []\n",
        "  for r in y_pred:\n",
        "    return_list.append(labels[r[0]]) #Contains labels(tags) for highest probability\n",
        "\n",
        "  return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "#Takes the tag returned by \"Pred_class\" , then uses it to randomly choose a response\n",
        "#corresponding to the same tag in \"intents.json\".\n",
        "  if len(intents_list) == 0:\n",
        "    result = \"Sorry! I don't understand\" #If intents_list is empty, then probability does not cross Threshold\n",
        "    #As a result, we put in \"Sorry\" as a response.\n",
        "  else:\n",
        "    tag = intents_list[0]\n",
        "    list_of_intents = intents_json[\"intents\"]\n",
        "    for i in list_of_intents:\n",
        "      if i[\"tag\"] == tag:\n",
        "        result = random.choice(i[\"responses\"])\n",
        "        break\n",
        "  return result\n",
        "  "
      ],
      "metadata": {
        "id": "aGfO60S-aT9D"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling relevant function and interacting with the Chatbot\n",
        "print(\"Press 0 if you don't want to chat with the ChatBot\")\n",
        "\n",
        "while True:\n",
        "  message = input(\"\")\n",
        "  if message == \"0\":\n",
        "    break\n",
        "  intents = pred_class(message, words, classes)\n",
        "  result = get_response(intents, data)\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "S0htctz1VHEU",
        "outputId": "224a03b8-244e-4319-8b11-14e47f80027f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Press 0 if you don't want to chat with the ChatBot\n",
            "hello\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Hi there, how can I help?\n",
            "hi\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Hello\n",
            "Hi there\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Hi there, how can I help?\n",
            "thanks\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Happy to help!\n",
            "Thank you\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "See you soon!\n",
            "Thanks\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Hi there, how can I help?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-5feeac7d13fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}